{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import elevator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN setup from\n",
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "torch_device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(torch_device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility setup\n",
    "Transition = namedtuple(\"Transition\",\n",
    "                      (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "class RingBuffer(deque):\n",
    "    def __init__(self, capacity):\n",
    "        super(RingBuffer, self).__init__(maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(list(self), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Q Agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, nfeats, nfloors, hidden_dim, outputs):\n",
    "        \"\"\"\n",
    "        LSTM based DQN model\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(nfeats, hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim * nfloors, outputs)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        lstm_out, _ = self.lstm(state)\n",
    "        state_relu = F.relu(lstm_out)\n",
    "        return self.head(state_relu.view(state_relu.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviromnet setup\n",
    "NUM_FLOORS = 10\n",
    "NUM_CARS = 1\n",
    "\n",
    "env = elevator.Elevator(NUM_FLOORS, NUM_CARS)\n",
    "\n",
    "# Passenger arrival following a\n",
    "# Poisson distribution. Passenger arrival count at each time\n",
    "# may thus be determined in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL params\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# DQN params\n",
    "HIDDEN_DIM = 3\n",
    "\n",
    "# Three actions, move up, move down, stay still\n",
    "n_actions = 3\n",
    "\n",
    "policy_net = DQN(env.nfeats, NUM_FLOORS, HIDDEN_DIM,\n",
    "                 n_actions).to(device)\n",
    "target_net = DQN(env.nfeats, NUM_FLOORS, HIDDEN_DIM,\n",
    "                 n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimized = optim.RMSprop(policy_net.parameters())\n",
    "memory = RingBuffer(100)\n",
    "\n",
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    \n",
    "    exploit = random.random()\n",
    "    eps_thresh = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if exploit > eps_thresh:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        \n",
    "        #Explore\n",
    "        return torch.tensor([[random.randrange(n_actions)]],\n",
    "                            device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s : s is not None,\n",
    "                                 batch.next_state)),\n",
    "                                 device=device, dtype=torch.unit8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                      if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    #Compute Q values based on policy net\n",
    "    sa_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(\n",
    "        non_final_next_states).max(1)[0].detach()\n",
    "    expected_sa_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = F.smooth_lq_loss(sa_values, expected_sa_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_epochs = 50\n",
    "#epoch_duration = 24 * 60\n",
    "num_epochs = 2\n",
    "epoch_duration = 100\n",
    "for ep_idx in range(num_epochs):\n",
    "    env.reset()\n",
    "    #TODO: Some initial people\n",
    "    state = env.state()\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    for t in range(epoch_duration):\n",
    "        action = select_action(state)\n",
    "        #TODO: Set what changes as result of action\n",
    "        reward = -1 * env.total_cost()\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        next_state = env.state()\n",
    "        next_state = torch.tensor([next_state], device=device,\n",
    "                                  dtype=torch.float)\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        \n",
    "        optimize_agent()\n",
    "        \n",
    "    if ep_idx % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
