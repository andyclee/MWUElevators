{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import elevator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN setup from\n",
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "torch_device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(torch_device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility setup\n",
    "Transition = namedtuple(\"Transition\",\n",
    "                      (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "class RingBuffer(deque):\n",
    "    def __init__(self, capacity):\n",
    "        super(RingBuffer, self).__init__(maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(list(self), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Q Agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, nfeats, nfloors, hidden_dim, outputs):\n",
    "        \"\"\"\n",
    "        LSTM based DQN model\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(nfeats, hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim * nfloors, outputs)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        lstm_out, _ = self.lstm(state)\n",
    "        state_relu = F.relu(lstm_out)\n",
    "        return self.head(state_relu.view(state_relu.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enviromnet setup\n",
    "\n",
    "# Four actions, move up, move down, stay (head up), stay (head down)\n",
    "n_actions = elevator.N_ACTIONS\n",
    "NUM_FLOORS = 10\n",
    "NUM_CARS = 1\n",
    "max_lambda = 5\n",
    "first_floor_func = elevator.gen_lambda_func(10, 5, max_lambda, 0.7)\n",
    "lambda_funcs = [ lambda x : max_lambda * 0.4 \\\n",
    "                for i in range(NUM_FLOORS - 1)]\n",
    "lambda_funcs.insert(0, first_floor_func)\n",
    "\n",
    "env = elevator.Elevator(NUM_FLOORS, NUM_CARS, lambda_funcs)\n",
    "\n",
    "# Passenger arrival following a\n",
    "# Poisson distribution. Passenger arrival count at each time\n",
    "# may thus be determined in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL params\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# DQN params\n",
    "HIDDEN_DIM = 3\n",
    "\n",
    "policy_net = DQN(env.nfeats, NUM_FLOORS, HIDDEN_DIM,\n",
    "                 n_actions).to(device)\n",
    "target_net = DQN(env.nfeats, NUM_FLOORS, HIDDEN_DIM,\n",
    "                 n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimized = optim.RMSprop(policy_net.parameters())\n",
    "memory = RingBuffer(100)\n",
    "\n",
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    \n",
    "    exploit = random.random()\n",
    "    eps_thresh = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if exploit > eps_thresh:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        \n",
    "        #Explore\n",
    "        return torch.tensor([[random.randrange(n_actions)\\\n",
    "                              for i in range(NUM_CARS)]],\n",
    "                              device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s : s is not None,\n",
    "                                 batch.next_state)),\n",
    "                                 device=device, dtype=torch.unit8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                      if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    #Compute Q values based on policy net\n",
    "    sa_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(\n",
    "        non_final_next_states).max(1)[0].detach()\n",
    "    expected_sa_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = F.smooth_lq_loss(sa_values, expected_sa_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Set changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-638991105dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MWUElevators/elevator.py\u001b[0m in \u001b[0;36mrun_iteration\u001b[0;34m(self, cur_time, actions)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mfloor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MWUElevators/elevator.py\u001b[0m in \u001b[0;36mrun_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mDisembark\u001b[0m \u001b[0mpassengers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mcost\u001b[0m \u001b[0mof\u001b[0m \u001b[0meveryone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \"\"\"\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpsgr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassengers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_floor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpsgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassengers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Set changed size during iteration"
     ]
    }
   ],
   "source": [
    "#num_epochs = 50\n",
    "#epoch_duration = 24 * 60\n",
    "num_epochs = 2\n",
    "epoch_duration = 100\n",
    "for ep_idx in range(num_epochs):\n",
    "    env.reset()\n",
    "\n",
    "    state = env.state()\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    for t in range(epoch_duration):\n",
    "        actions = select_action(state)\n",
    "        env.run_iteration(t, actions[0])\n",
    "        reward = -1 * env.total_cost()\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        next_state = env.state()\n",
    "        next_state = torch.tensor([next_state], device=device,\n",
    "                                  dtype=torch.float)\n",
    "        memory.push(state, actions, next_state, reward)\n",
    "        state = next_state\n",
    "        \n",
    "        optimize_agent()\n",
    "        \n",
    "    if ep_idx % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
